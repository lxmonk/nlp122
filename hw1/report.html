<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
               "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>NLP12 Assignment 1: Parts of Speech Tagging</title>
<meta http-equiv="Content-Type" content="text/html;charset=iso-8859-1"/>
<meta name="title" content="NLP12 Assignment 1: Parts of Speech Tagging"/>
<meta name="generator" content="Org-mode"/>
<meta name="generated" content="2012-03-19 Mon"/>
<meta name="author" content="Aviad Reich, ID 052978509"/>
<meta name="description" content=""/>
<meta name="keywords" content=""/>
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  html { font-family: Times, serif; font-size: 12pt; }
  .title  { text-align: center; }
  .todo   { color: red; }
  .done   { color: green; }
  .tag    { background-color: #add8e6; font-weight:normal }
  .target { }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  {margin-left:auto; margin-right:0px;  text-align:right;}
  .left   {margin-left:0px;  margin-right:auto; text-align:left;}
  .center {margin-left:auto; margin-right:auto; text-align:center;}
  p.verse { margin-left: 3% }
  pre {
	border: 1pt solid #AEBDCC;
	background-color: #F3F5F7;
	padding: 5pt;
	font-family: courier, monospace;
        font-size: 90%;
        overflow:auto;
  }
  table { border-collapse: collapse; }
  td, th { vertical-align: top;  }
  th.right  { text-align:center;  }
  th.left   { text-align:center;   }
  th.center { text-align:center; }
  td.right  { text-align:right;  }
  td.left   { text-align:left;   }
  td.center { text-align:center; }
  dt { font-weight: bold; }
  div.figure { padding: 0.5em; }
  div.figure p { text-align: center; }
  div.inlinetask {
    padding:10px;
    border:2px solid gray;
    margin:10px;
    background: #ffffcc;
  }
  textarea { overflow-x: auto; }
  .linenr { font-size:smaller }
  .code-highlighted {background-color:#ffff00;}
  .org-info-js_info-navigation { border-style:none; }
  #org-info-js_console-label { font-size:10px; font-weight:bold;
                               white-space:nowrap; }
  .org-info-js_search-highlight {background-color:#ffff00; color:#000000;
                                 font-weight:bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="nlp.css" media="all" />
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://orgmode.org/mathjax/MathJax.js">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
        //  config: ["MMLorHTML.js"], jax: ["input/TeX"],
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>

<div id="preamble">

</div>

<div id="content">
<h1 class="title">NLP12 Assignment 1: Parts of Speech Tagging</h1>

<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#sec-1">1 Data Exploration [1/2]</a>
<ul>
<li><a href="#sec-1-1">1.1 Gathering and cleaning up data</a>
<ul>
<li><a href="#sec-1-1-1">1.1.1 Manual Tagging:</a></li>
</ul>
</li>
<li><a href="#sec-1-2">1.2 Gathering basic statistics</a></li>
<li><a href="#sec-1-3">1.3 Is there a correlation between word size or frequency and ambiguity level?</a></li>
</ul>
</li>
<li><a href="#sec-2">2 Unigram and Affix Tagger [0/2]</a>
<ul>
<li><a href="#sec-2-1">2.1 Unigram Tagger</a></li>
<li><a href="#sec-2-2">2.2 Affix Tagger</a></li>
</ul>
</li>
<li><a href="#sec-3">3 Fine-grained Error Analysis [1/5]</a>
<ul>
<li><a href="#sec-3-1">3.1 Known vs. Unknown Accuracy</a></li>
<li><a href="#sec-3-2">3.2 Per Tag Precision and Recall</a></li>
<li><a href="#sec-3-3">3.3 Confusion Matrix</a></li>
<li><a href="#sec-3-4">3.4 SensAitivity to the Size and Structure of the Training Set: Cross-Validation</a></li>
<li><a href="#sec-3-5">3.5 Stratified Samples</a></li>
</ul>
</li>
</ul>
</div>
</div>


<p>
NOTES: In this assignment, the new "<a href="http://docs.python-requests.org/en/latest/index.html#">requests</a>" python library was used,
and not <code>urllib</code>, as suggested in the tutorial. The web (and python)
have evolved since python 2.5, used in the nltk book, was released
in 2008. <br/>
Additionally, since <code>xgoogle</code> seemed to return very few results (1-3 on
most searches), I used <a href="http://breakingcode.wordpress.com/2010/06/29/google-search-python/">this script</a> instead.
</p>

<div id="outline-container-1" class="outline-2">
<h2 id="sec-1"><span class="section-number-2">1</span> Data Exploration [1/2]</h2>
<div class="outline-text-2" id="text-1">


</div>

<div id="outline-container-1-1" class="outline-3">
<h3 id="sec-1-1"><span class="section-number-3">1.1</span> Gathering and cleaning up data</h3>
<div class="outline-text-3" id="text-1-1">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-04-30 Mon 22:44</span></span><br/>
</p>
<ol>
<li><b>Errors met while dealing with the Google engine:</b> <br/>
   Using the xgoogle library, only produced 1-3 result URLs for the
   searches, so I tried several alternatives: the python-duckduckgo
   library, but this only produced 1 result per query (duckduckgo only
   supplies a 'zero-click' api). Eventually, I used <a href="http://breakingcode.wordpress.com/2010/06/29/google-search-python/">this script</a>, which
   worked perfectly.

</li>
<li><b>Errors met while downloading the material from the Google hits:</b> <br/>
   I had no trouble, and was using the "requests" library.

</li>
<li><b>Errors met while cleaning up the HTML pages:</b> <br/>
   The built-in <code>nltk.clean_html</code> function did only a mediocre job
   cleaning the contents of the web pages, leaving in some irrelevant
   strings. Using the justext library worked perfectly.

</li>
<li><b>Errors met while segmenting the text into sentences and words:</b> <br/>
   Headlines were not segmented in to different sentences, but were
   instead included in the following sentence.
   The em-dash (represented as "- -", without spaces) was not regarded as separating
   between words when it should have been. For example, in: "sliding/N
   movement- -the/DET days/N of/P heaving/VG" we can see the words
   'movement' and 'the' <b>not</b> separated.

</li>
<li><b>Errors met by the automatic tagger:</b> <br/>
   When fed sentences that were correctly segmented to to words, and
   were also complete, and valid - I had no corrections to
   offer. Perhaps it's my lack of competence as a tagger for English
   words.. 
</li>
</ol>



</div>

<div id="outline-container-1-1-1" class="outline-4">
<h4 id="sec-1-1-1"><span class="section-number-4">1.1.1</span> Manual Tagging:</h4>
<div class="outline-text-4" id="text-1-1-1">

<p> Review the tagging of the new text separately (2 analyses) and
 compare your tagging results. Report the list of words on which your
 2 manual tagging decisions are different (write a function to
 compare two taggings of the same text saved in 2 different tagged
 files.) Show the differences between each of your tags and the tags
 produced by the automatic tagger. Report how long it took you to
 check the tagging of 50 sentences.  
</p>

<p>
This is the code I used:
</p>


<pre class="example">from cPickle import dump
import justext
import nltk
import requests
from google import search

def main():
    pages_used = 0
    for url in search('aeron chair', stop=30):
        text = ""
        url_used = False
        html = requests.get(url).content
        paragraphs = justext.justext(html, justext.get_stoplist('English'))
        for paragraph in paragraphs:
            if paragraph['class'] == 'good':
                if not url_used:
                    url_used = True
                    pages_used += 1
                text += (paragraph['text'] + '\n')
        if url_used:
            print 'now analyzing text from: {}'.format(url)
            rawfile = 'rawfile_{}.txt'.format(pages_used) 
            with file(rawfile, 'wb') as f:
                f.write(text)                              # save raw text to file

            sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
            sents = sent_tokenizer.tokenize(text)
            sentsfile = 'sentsfile_{}.pkl'.format(pages_used)
            dump(sents, file(sentsfile, 'wb'), protocol=2) # pickle the sentences to a file

            tokenized_sents = []
            tokenized_sents += [nltk.word_tokenize(sent) for sent in sents]
            tokfile = 'tokfile_{}.pkl'.format(pages_used)
            dump(tokenized_sents, file(tokfile, 'wb'), protocol=2)  # pickle the tokenized_sents to a file

            tagged_sents = [_tag(sent) for sent in tokenized_sents]
            tagged_text = ""
            for sent in tagged_sents:
                tagged_text += ('\n\n\t' + # sentence seperator
                                # create whitespace-separated 'word/tag' sentences
                                ' '.join(['/'.join(word_tag_tuple) for word_tag_tuple in sent]) +
                                '\n') # newline after sentence

            corpfile = 'corpfile_{}.txt'.format(pages_used)
            with file(corpfile, 'wb') as f:
                f.write(tagged_text)

    return 0

def _tag(sent):
    """
    This is taken from http://goo.gl/TxTyq (short for
    stackoverflow.com/...) with minor changes.
    This function returns the inputed 'sent' as tagged by nltk.pos_tag
    converted to Brown simplified tags.
    """
    from nltk.tag.simplify import simplify_brown_tag
    tagged_sent = nltk.pos_tag(sent) 
    simplified = [(word, simplify_brown_tag(tag)) for word, tag in tagged_sent]
    return simplified

if __name__ == '__main__':
    main()
</pre>


</div>
</div>

</div>

<div id="outline-container-1-2" class="outline-3">
<h3 id="sec-1-2"><span class="section-number-3">1.2</span> Gathering basic statistics</h3>
<div class="outline-text-3" id="text-1-2">

<p>   <b>IMPORTANT:</b> the code snippets in this section use the <code>tagged_words</code>
   corpus (NOT <code>tagged_sents</code>).
</p>

<p>
   To create Figure 1:
</p>

<div class="figure">
<p><img src="ambiguity-fig1.png" width="800" alt="ambiguity-fig1.png" /></p>
<p><b>Figure 1</b></p>
</div>

<p>   
   I used the following code:
</p>


<pre class="example">from __future__ import division
from collections import defaultdict, Counter
from numpy import log
import nltk
import pylab


def PlotNumberOfTags(corpus):
    word_tag_dict = defaultdict(set)

    for (word, tag) in corpus:
        word_tag_dict[word].add(tag)
    # using Counter for efficiency (leaner than FreqDist)
    C = Counter(len(val) for val in word_tag_dict.itervalues())

    pylab.subplot(211)
    pylab.plot(C.keys(), C.values(), '-go', label='Linear Scale')
    pylab.suptitle('Word Ambiguity:')
    pylab.title('Number of Words by Possible Tag Number')
    pylab.box('off')                 # for better appearance
    pylab.grid('on')                 # for better appearance
    pylab.ylabel('Words With This Number of Tags (Linear)')
    pylab.legend(loc=0)
    # add value tags
    for x,y in zip(C.keys(), C.values()):
        pylab.annotate(str(y), (x,y + 0.5))

    pylab.subplot(212)
    pylab.plot(C.keys(), C.values(), '-bo', label='Logarithmic Scale')
    pylab.yscale('log') # to make the graph more readable, for the log graph version
    pylab.box('off')                 # for better appearance
    pylab.grid('on')                 # for better appearance
    pylab.xlabel('Number of Tags per Word')
    pylab.ylabel('Words With This Number of Tags (Log)')
    pylab.legend(loc=0)
    # add value tags
    for x,y in zip(C.keys(), C.values()):
        pylab.annotate(str(y), (x,y + 0.5))
</pre>


<p>   
   For the requested functions, This code was used:
</p>


<pre class="example">    pylab.ylabel('Words With This Number of Tags (Log)')
    pylab.legend(loc=0)
    # add value tags
    for x,y in zip(C.keys(), C.values()):
        pylab.annotate(str(y), (x,y + 0.5))

    pylab.show()




def MostAmbiguousWords(corpus, N):
    word_tag_dict = defaultdict(set)

    for (word, tag) in corpus:
        word_tag_dict[word].add(tag)

    filtered_tagged_words = [(word, tag) for (word, tag) in corpus if len(word_tag_dict[word]) &gt; N]
    return nltk.ConditionalFreqDist(filtered_tagged_words)

def TestMostAmbiguousWords(cfd, N):
    all_good = True
    for word in cfd.conditions():
        all_good &amp;= (len(cfd[word]) &gt; N)

    if all_good:
        print 'All words occur with more than {} tags.'.format(N)
    else:
        print 'ERROR: Some words occur with less (or exactly) {} tags'.format(N)

def ShowExamples(word, cfd, corpus):
    for tag in cfd[word].keys():
        print '\'{}\' as {}: {}\n'.format(word, tag, example(word, tag, corpus))

</pre>


</div>

</div>

<div id="outline-container-1-3" class="outline-3">
<h3 id="sec-1-3"><span class="section-number-3">1.3</span> Is there a correlation between word size or frequency and ambiguity level?</h3>
<div class="outline-text-3" id="text-1-3">

<p>   To try and answer this question, I plotted the requested 3D graph
   (figure 2a):
</p>
<div class="figure">
<p><img src="ambiguity1x2.png" width="950" alt="ambiguity1x2.png" /></p>
<p><b>Figure 2a</b></p>
</div>

<p>
In order to better understand it, I also looked at the 3 2D
Projections of it (figure 2b), and used a logarithmic scale for word
frequency (figure 3c):
</p>
<div class="figure">
<p><img src="ambiguity2x2.png" width="950" alt="ambiguity2x2.png" /></p>
<p><b>Figure 2b - linear projections</b></p>
</div>


<div class="figure">
<p><img src="ambiguity2x2-log.png" width="950" alt="ambiguity2x2-log.png" /></p>
<p><b>Figure 2c - logarithmic projections</b></p>
</div>

<p>
Two correlations are clearly visible: <b>word frequency - word length</b> (as
discussed in class, due to "evolution" maybe), and <b>word ambiguity - word length</b> (probably not a strictly linear correlation).<br/>
It looks probable, that there is also some correlation between <b>word ambiguity - word frequency</b>, however, it's difficult to decide whether
this correlation will exist after taking into account the previous two
correlation, since they appear to be much more significant.<br/>
It is also worth noting that  all these observations were made with
only looking at the graphs, and might be wrong when tested for
statistical significance.
</p>

<p>
This is the code used (with minor changes or commenting
out, to create the different graphs):
</p>


<pre class="example">        print '\'{}\' as {}: {}\n'.format(word, tag, example(word, tag, corpus))


def example(word, tag, corpus):
    idx = corpus.index((word, tag))
    sent = corpus[idx-10:idx] + [(word.upper(), tag)] + corpus[idx+1:idx+11]
    return ' '.join(word for (word, tag) in sent)




def correl_plot3D(corpus):
    from mpl_toolkits.mplot3d import Axes3D

    word_tag_dict = defaultdict(set)
    for (word, tag) in corpus:
        word_tag_dict[word].add(tag)

    raw_wordlist = [word for (word, tag) in corpus]
    wordset = set(raw_wordlist)
    wordlist = list(wordset)
    word_fd = nltk.FreqDist(raw_wordlist)

    fig = pylab.figure(figsize=(15,15))
    ax = fig.add_subplot(224, projection='3d') # 224
    xs = [len(w) for w in wordlist]
    ys = [word_fd[w] for w in wordlist]
    zs = [len(word_tag_dict[w]) for w in wordlist]
    ax.scatter(xs, ys, zs)
    ax.set_xlabel('word length (charachters)')
    ax.set_ylabel('word frequency')
    ax.set_zlabel('word ambiguity')

    pylab.subplot(221)
    pylab.yscale('log')
    pylab.ylim(ymin=1, ymax=100000)
    pylab.scatter(xs, ys)
    pylab.title('word length - word freq (log)')
    pylab.xlabel('word length')
    pylab.ylabel('word freq (log)')

    pylab.subplot(222)
    pylab.xscale('log')
    pylab.xlim(xmin=1, xmax=100000)
    pylab.scatter(ys, zs)
    pylab.title('word freq (log) - word ambiguity')
    pylab.xlabel('word freq (log)')
    pylab.ylabel('word ambiguity')

    pylab.subplot(223)
</pre>

</div>
</div>

</div>

<div id="outline-container-2" class="outline-2">
<h2 id="sec-2"><span class="section-number-2">2</span> Unigram and Affix Tagger [0/2]</h2>
<div class="outline-text-2" id="text-2">


</div>

<div id="outline-container-2-1" class="outline-3">
<h3 id="sec-2-1"><span class="section-number-3">2.1</span> Unigram Tagger</h3>
<div class="outline-text-3" id="text-2-1">

<p>   This is the code for the Unigram Tagger:
</p>


<pre class="example">    pylab.xlabel('word ambiguity')
    pylab.ylabel('word length')

    pylab.show()



from nltk.tag.api import TaggerI
import nltk

class MyUnigramTagger(TaggerI):
    def __init__(self, train=None, model=None,
                 backoff=None, cutoff=0, verbose=False):
        if type(train[0]) == tuple:
            pass
        elif type(train[0][0]) == tuple:
            train = flatten(train)            
        self.cfd = nltk.ConditionalFreqDist(train)
        self.default_tag = nltk.FreqDist(tag for (word, tag) in train).max()
        self.wordset = set(word for (word, tag) in train)

    def tag(self, tokens):
        # docs inherited from TaggerI
        return zip(tokens, [self.cfd[word].max()
                            if word in self.wordset
                            else self.default_tag
                            for word in tokens])

# This really should have come with either itertools or standard lib..
from itertools import chain
</pre>

</div>

</div>

<div id="outline-container-2-2" class="outline-3">
<h3 id="sec-2-2"><span class="section-number-3">2.2</span> Affix Tagger</h3>
<div class="outline-text-3" id="text-2-2">

<p>This is the code used:
</p>


<pre class="example">from nltk.tag import AffixTagger

class MyAffixTagger(AffixTagger):
    def __init__(self, train=None, model=None, affix_length=-3,
                 min_stem_length=2, backoff=None, cutoff=0, verbose=False,
                 H_param=0):
        self.H_param = H_param
        AffixTagger.__init__(self, train, model, affix_length,
                             min_stem_length, backoff, cutoff, verbose)


    def _train(self, tagged_corpus, cutoff=0, verbose=False):
        """
        Initialize this ContextTagger's ``_context_to_tag`` table
        based on the given training data.  In particular, for each
        context ``c`` in the training data, set
        ``_context_to_tag[c]`` to the most frequent tag for that
        context.  However, exclude any contexts that are already
        tagged perfectly by the backoff tagger(s).

        The old value of ``self._context_to_tag`` (if any) is discarded.

        :param tagged_corpus: A tagged corpus.  Each item should be
            a list of (word, tag tuples.
        :param cutoff: If the most likely tag for a context occurs
            fewer than cutoff times, then exclude it from the
            context-to-tag table for the new tagger.
        """

        token_count = hit_count = 0

        # A context is considered 'useful' if it's not already tagged
        # perfectly by the backoff tagger.
        useful_contexts = set()

        # Count how many times each tag occurs in each context.
        fd = nltk.ConditionalFreqDist()
        for sentence in tagged_corpus:
            tokens, tags = zip(*sentence)
            for index, (token, tag) in enumerate(sentence):
                # Record the event.
                token_count += 1
                context = self.context(tokens, index, tags[:index])
                if context is None: continue
                fd[context].inc(tag)
                # If the backoff got it wrong, this context is useful:
                if (self.backoff is None or
                    tag != self.backoff.tag_one(tokens, index, tags[:index])):
                    useful_contexts.add(context)

        # Build the context_to_tag table -- for each context, figure
        # out what the most likely tag is.  Only include contexts that
        # we've seen at least `cutoff` times.
        ### best_preds = []
        for context in useful_contexts:
            best_tag = fd[context].max()
            hits = fd[context][best_tag]
            if hits &gt; cutoff and self.H(fd[context]) &gt; self.H_param:
                self._context_to_tag[context] = best_tag
                hit_count += hits

           ###      prediction = fd[context][best_tag] / fd[context].N()
        ###         if prediction &gt; 0.8:
        ###             best_preds.append((prediction, context))
        ### for prediction, context in sorted(best_preds, reverse=True):
        ###     print '{:3} gives {:.3f} percent prediction as {}'.format(context,
        ###                                                               prediction * 100,
        ###                                                               fd[context].max())

    def H(self, p):
        # we already used 'from __future__ import division''
        N = p.N()               # total num of items seen by the fd
        sum = 0
        for k,v in p.iteritems():
            Px = v / N          # future division
            logPx = log(Px)
            PxXlogPx = 0 if Px == 0 else Px * logPx 
            sum -= PxXlogPx
        return sum



    def optimize_parameter(self):
        corpus = nltk.corpus.brown.tagged_sents(simplify_tags=True)
        corpus_len = len(corpus)
        trainset = corpus[:int(corpus_len * 8 / 10)] # this is ok, it's an int
        develset = corpus[int(corpus_len * 8 / 10):int(corpus_len * 9 / 10)]
        testset =  corpus[int(corpus_len * 9 / 10):]
        options = []            # list of (accuracy, param_val, context_to_tag_table) tuples
        for cutoff_candidate in range(1,5) + range(5,10,2) + range(10,51,10):
            print 'optimizing cutoff parameter: trying {:2}'.format(cutoff_candidate),
            self._context_to_tag.clear() # clear context_to_tag table
            self._train(trainset, cutoff=cutoff_candidate)
            score = self.evaluate(develset)
            print ' ---&gt; it scored {:.5f}'.format(score)
            options.append((score, cutoff_candidate, self._context_to_tag))

        _, choosen_cutoff, self._context_to_tag = max(options)
        print ('choosen cutoff value is {}, which scores {} on the '
               'testset.'.format(choosen_cutoff,
</pre>

<p>
To produce this output (<code>H_param</code> was 0):
</p>



<pre class="example">optimizing cutoff parameter: trying 1   ---&gt; it scored 0.19049
optimizing cutoff parameter: trying 2   ---&gt; it scored 0.19033
optimizing cutoff parameter: trying 3   ---&gt; it scored 0.19027
optimizing cutoff parameter: trying 4   ---&gt; it scored 0.19005
optimizing cutoff parameter: trying 5   ---&gt; it scored 0.18988
optimizing cutoff parameter: trying 6   ---&gt; it scored 0.18979
optimizing cutoff parameter: trying 7   ---&gt; it scored 0.18963
optimizing cutoff parameter: trying 8   ---&gt; it scored 0.18938
optimizing cutoff parameter: trying 9   ---&gt; it scored 0.18921
optimizing cutoff parameter: trying 10  ---&gt; it scored 0.18917
optimizing cutoff parameter: trying 15  ---&gt; it scored 0.18848
optimizing cutoff parameter: trying 20  ---&gt; it scored 0.18722
optimizing cutoff parameter: trying 25  ---&gt; it scored 0.18611
optimizing cutoff parameter: trying 30  ---&gt; it scored 0.18518
optimizing cutoff parameter: trying 35  ---&gt; it scored 0.18367
optimizing cutoff parameter: trying 40  ---&gt; it scored 0.18211
optimizing cutoff parameter: trying 45  ---&gt; it scored 0.18129

choosen cutoff value is 1, which scores 0.182246809581 on the testset.
</pre>


<p>
Later, I added this function to optimize the <code>H_param</code> AND the cutoff
value:
</p>


<pre class="example">corp = nltk.corpus.brown.tagged_sents(simplify_tags=True)
for h in linspace(0,1,10)[:-1]: # remove the 1.0
    print
    print 'optimize_h_param: H_param={:f}'.format(h)
    affix = MyAffixTagger(corp, H_param=h)
    affix.optimize_parameter()


</pre>


<p>
This produced these values:
</p>


<pre class="example">optimize_h_param: H_param=0.000000
optimizing cutoff parameter: trying  1  ---&gt; it scored 0.19049
optimizing cutoff parameter: trying  2  ---&gt; it scored 0.19033
optimizing cutoff parameter: trying  3  ---&gt; it scored 0.19027
optimizing cutoff parameter: trying  4  ---&gt; it scored 0.19005
optimizing cutoff parameter: trying  5  ---&gt; it scored 0.18988
optimizing cutoff parameter: trying  7  ---&gt; it scored 0.18963
optimizing cutoff parameter: trying  9  ---&gt; it scored 0.18921
optimizing cutoff parameter: trying 10  ---&gt; it scored 0.18917
optimizing cutoff parameter: trying 20  ---&gt; it scored 0.18722
optimizing cutoff parameter: trying 30  ---&gt; it scored 0.18518
optimizing cutoff parameter: trying 40  ---&gt; it scored 0.18211
optimizing cutoff parameter: trying 50  ---&gt; it scored 0.18028
choosen cutoff value is 1, which scores 0.181345738773 on the testset.

optimize_h_param: H_param=0.111111
optimizing cutoff parameter: trying  1  ---&gt; it scored 0.17432
optimizing cutoff parameter: trying  2  ---&gt; it scored 0.17416
optimizing cutoff parameter: trying  3  ---&gt; it scored 0.17410
optimizing cutoff parameter: trying  4  ---&gt; it scored 0.17388
optimizing cutoff parameter: trying  5  ---&gt; it scored 0.17371
optimizing cutoff parameter: trying  7  ---&gt; it scored 0.17346
optimizing cutoff parameter: trying  9  ---&gt; it scored 0.17305
optimizing cutoff parameter: trying 10  ---&gt; it scored 0.17300
optimizing cutoff parameter: trying 20  ---&gt; it scored 0.17105
optimizing cutoff parameter: trying 30  ---&gt; it scored 0.16902
optimizing cutoff parameter: trying 40  ---&gt; it scored 0.16594
optimizing cutoff parameter: trying 50  ---&gt; it scored 0.16419
choosen cutoff value is 1, which scores 0.164895957754 on the testset.

optimize_h_param: H_param=0.222222
optimizing cutoff parameter: trying  1  ---&gt; it scored 0.16144
optimizing cutoff parameter: trying  2  ---&gt; it scored 0.16128
optimizing cutoff parameter: trying  3  ---&gt; it scored 0.16122
optimizing cutoff parameter: trying  4  ---&gt; it scored 0.16100
optimizing cutoff parameter: trying  5  ---&gt; it scored 0.16083
optimizing cutoff parameter: trying  7  ---&gt; it scored 0.16058
optimizing cutoff parameter: trying  9  ---&gt; it scored 0.16017
optimizing cutoff parameter: trying 10  ---&gt; it scored 0.16012
optimizing cutoff parameter: trying 20  ---&gt; it scored 0.15824
optimizing cutoff parameter: trying 30  ---&gt; it scored 0.15626
optimizing cutoff parameter: trying 40  ---&gt; it scored 0.15324
optimizing cutoff parameter: trying 50  ---&gt; it scored 0.15193
choosen cutoff value is 1, which scores 0.151463716184 on the testset.

optimize_h_param: H_param=0.333333
optimizing cutoff parameter: trying  1  ---&gt; it scored 0.14877
optimizing cutoff parameter: trying  2  ---&gt; it scored 0.14861
optimizing cutoff parameter: trying  3  ---&gt; it scored 0.14855
optimizing cutoff parameter: trying  4  ---&gt; it scored 0.14833
optimizing cutoff parameter: trying  5  ---&gt; it scored 0.14816
optimizing cutoff parameter: trying  7  ---&gt; it scored 0.14791
optimizing cutoff parameter: trying  9  ---&gt; it scored 0.14755
optimizing cutoff parameter: trying 10  ---&gt; it scored 0.14752
optimizing cutoff parameter: trying 20  ---&gt; it scored 0.14600
optimizing cutoff parameter: trying 30  ---&gt; it scored 0.14449
optimizing cutoff parameter: trying 40  ---&gt; it scored 0.14162
optimizing cutoff parameter: trying 50  ---&gt; it scored 0.14032
choosen cutoff value is 1, which scores 0.138670606232 on the testset.

optimize_h_param: H_param=0.444444
optimizing cutoff parameter: trying  1  ---&gt; it scored 0.13839
optimizing cutoff parameter: trying  2  ---&gt; it scored 0.13823
optimizing cutoff parameter: trying  3  ---&gt; it scored 0.13817
optimizing cutoff parameter: trying  4  ---&gt; it scored 0.13795
optimizing cutoff parameter: trying  5  ---&gt; it scored 0.13778
optimizing cutoff parameter: trying  7  ---&gt; it scored 0.13757
optimizing cutoff parameter: trying  9  ---&gt; it scored 0.13737
optimizing cutoff parameter: trying 10  ---&gt; it scored 0.13733
optimizing cutoff parameter: trying 20  ---&gt; it scored 0.13592
optimizing cutoff parameter: trying 30  ---&gt; it scored 0.13453
optimizing cutoff parameter: trying 40  ---&gt; it scored 0.13247
optimizing cutoff parameter: trying 50  ---&gt; it scored 0.13129
choosen cutoff value is 1, which scores 0.129722763563 on the testset.

optimize_h_param: H_param=0.555556
optimizing cutoff parameter: trying  1  ---&gt; it scored 0.12325
optimizing cutoff parameter: trying  2  ---&gt; it scored 0.12308
optimizing cutoff parameter: trying  3  ---&gt; it scored 0.12303
optimizing cutoff parameter: trying  4  ---&gt; it scored 0.12292
optimizing cutoff parameter: trying  5  ---&gt; it scored 0.12280
optimizing cutoff parameter: trying  7  ---&gt; it scored 0.12260
optimizing cutoff parameter: trying  9  ---&gt; it scored 0.12240
optimizing cutoff parameter: trying 10  ---&gt; it scored 0.12236
optimizing cutoff parameter: trying 20  ---&gt; it scored 0.12110
optimizing cutoff parameter: trying 30  ---&gt; it scored 0.11981
optimizing cutoff parameter: trying 40  ---&gt; it scored 0.11816
optimizing cutoff parameter: trying 50  ---&gt; it scored 0.11714
choosen cutoff value is 1, which scores 0.114760797133 on the testset.

optimize_h_param: H_param=0.666667
optimizing cutoff parameter: trying  1  ---&gt; it scored 0.10689
optimizing cutoff parameter: trying  2  ---&gt; it scored 0.10685
optimizing cutoff parameter: trying  3  ---&gt; it scored 0.10684
optimizing cutoff parameter: trying  4  ---&gt; it scored 0.10678
optimizing cutoff parameter: trying  5  ---&gt; it scored 0.10667
optimizing cutoff parameter: trying  7  ---&gt; it scored 0.10648
optimizing cutoff parameter: trying  9  ---&gt; it scored 0.10636
optimizing cutoff parameter: trying 10  ---&gt; it scored 0.10634
optimizing cutoff parameter: trying 20  ---&gt; it scored 0.10531
optimizing cutoff parameter: trying 30  ---&gt; it scored 0.10454
optimizing cutoff parameter: trying 40  ---&gt; it scored 0.10313
optimizing cutoff parameter: trying 50  ---&gt; it scored 0.10246
choosen cutoff value is 1, which scores 0.100417007188 on the testset.

optimize_h_param: H_param=0.777778
optimizing cutoff parameter: trying  1  ---&gt; it scored 0.08989
optimizing cutoff parameter: trying  2  ---&gt; it scored 0.08989
optimizing cutoff parameter: trying  3  ---&gt; it scored 0.08989
optimizing cutoff parameter: trying  4  ---&gt; it scored 0.08984
optimizing cutoff parameter: trying  5  ---&gt; it scored 0.08974
optimizing cutoff parameter: trying  7  ---&gt; it scored 0.08958
optimizing cutoff parameter: trying  9  ---&gt; it scored 0.08950
optimizing cutoff parameter: trying 10  ---&gt; it scored 0.08947
optimizing cutoff parameter: trying 20  ---&gt; it scored 0.08878
optimizing cutoff parameter: trying 30  ---&gt; it scored 0.08831
optimizing cutoff parameter: trying 40  ---&gt; it scored 0.08693
optimizing cutoff parameter: trying 50  ---&gt; it scored 0.08648
choosen cutoff value is 3, which scores 0.08349573563 on the testset.

optimize_h_param: H_param=0.888889
optimizing cutoff parameter: trying  1  ---&gt; it scored 0.06796
optimizing cutoff parameter: trying  2  ---&gt; it scored 0.06796
optimizing cutoff parameter: trying  3  ---&gt; it scored 0.06796
optimizing cutoff parameter: trying  4  ---&gt; it scored 0.06794
optimizing cutoff parameter: trying  5  ---&gt; it scored 0.06786
optimizing cutoff parameter: trying  7  ---&gt; it scored 0.06770
optimizing cutoff parameter: trying  9  ---&gt; it scored 0.06762
optimizing cutoff parameter: trying 10  ---&gt; it scored 0.06759
optimizing cutoff parameter: trying 20  ---&gt; it scored 0.06704
optimizing cutoff parameter: trying 30  ---&gt; it scored 0.06663
optimizing cutoff parameter: trying 40  ---&gt; it scored 0.06548
optimizing cutoff parameter: trying 50  ---&gt; it scored 0.06522
choosen cutoff value is 3, which scores 0.0618281259823 on the testset.

</pre>

<p>
We can clearly see that cutting off according to ANY criteria, causes
a degradation in the tagger's performance, except for the case when
<code>H_param</code> was high (\(\frac{7}{9}\), \(\frac{8}{9}\)), where cutoff values
of 1-3 are equally good.
</p>

<ol>
<li><b>Does entropy filtering improve accuracy?</b>
   It does not (!!)

</li>
<li><b>How do you determine the range of values to test for the cutoff?</b>
   I used the values 1-5, 7,9, 10, 20, 30, 40, 50. This was not taken
   from the corpus, but seemed to give more than reasonable cover for
   several corpora.

</li>
<li><b>Is the accuracy value evolving in a predictable manner as the cutoff varies?</b>
   It decreases as the cutoff increases.

</li>
<li><b>Describe the list of suffixes that are good tag predictors &ndash; are    you surprised by what you observe?</b>
</li>
</ol>






<pre class="example">eir gives 99.963 percent prediction as PRO
uld gives 99.942 percent prediction as MOD
ods gives 99.735 percent prediction as N
rld gives 99.624 percent prediction as N
iam gives 99.444 percent prediction as NP
ths gives 99.413 percent prediction as N
tel gives 99.379 percent prediction as N
ups gives 99.363 percent prediction as N
ror gives 99.306 percent prediction as N
cil gives 99.296 percent prediction as N
're gives 99.288 percent prediction as PRO
rgy gives 99.180 percent prediction as N
hip gives 99.135 percent prediction as N
tem gives 99.057 percent prediction as N
ege gives 98.997 percent prediction as N
tee gives 98.985 percent prediction as N
umn gives 98.936 percent prediction as N
try gives 98.825 percent prediction as N
ogy gives 98.805 percent prediction as N
hor gives 98.780 percent prediction as N
ity gives 98.778 percent prediction as N
dex gives 98.765 percent prediction as N
arp gives 98.649 percent prediction as ADJ
hod gives 98.611 percent prediction as N
yne gives 98.485 percent prediction as NP
not gives 98.473 percent prediction as MOD
nst gives 98.425 percent prediction as P
ons gives 98.391 percent prediction as N
oms gives 98.378 percent prediction as N
elf gives 98.328 percent prediction as PRO
ife gives 98.230 percent prediction as N
bol gives 98.214 percent prediction as N
gth gives 98.127 percent prediction as N
dar gives 98.077 percent prediction as N
lue gives 98.058 percent prediction as N
wly gives 98.000 percent prediction as ADV
sis gives 97.950 percent prediction as N
hts gives 97.947 percent prediction as N
rey gives 97.917 percent prediction as NP
ski gives 97.778 percent prediction as NP
ffs gives 97.778 percent prediction as N
ols gives 97.612 percent prediction as N
ool gives 97.543 percent prediction as N
</pre>


<p>
I can see why some of these would be good predictors, especially for
the Nouns I (think I) recognize, such as "ool", "ols", "ogy", and why "'re",
but for most of them - I can't really imagine why they are so good.
I have to admit that I don't recognize any of these as a suffix I
studies in linguistics.
</p>
</div>
</div>

</div>

<div id="outline-container-3" class="outline-2">
<h2 id="sec-3"><span class="section-number-2">3</span> Fine-grained Error Analysis [1/5]</h2>
<div class="outline-text-2" id="text-3">


</div>

<div id="outline-container-3-1" class="outline-3">
<h3 id="sec-3-1"><span class="section-number-3">3.1</span> Known vs. Unknown Accuracy</h3>
<div class="outline-text-3" id="text-3-1">

<p>    <span class="timestamp-wrapper"><span class="timestamp-kwd">CLOSED: </span> <span class="timestamp">2012-05-03 Thu 01:29</span></span><br/>
   I propose to identify known words as words that appeared more than
   \(N\) times in the training corpus, where \(N&gt;0\) (I'll use \(N=1\), but that's just
   one option).
</p>
<p>
   I suggest adding the code (for \(N=1\)): <br/>
   <code>self.wordset = set(word for (word, tag) in sentence in train)</code>
</p>
<p>   
   To every tagger. This could be used in the <code>evaluate2</code> method, in
   the following manner:
</p>


<pre class="example">
def evaluate2(self, gold):
    """
    This is part of nlp12 course, hw 1.
    """

    from itertools import ifilter, ifilterfalse

    tagged_sents = self.batch_tag([untag(sent) for sent in gold])
    gold_tokens = sum(gold, []) 
    test_tokens = sum(tagged_sents, [])
    known_gold_tokens = ifilter(lambda word, tag: word in self.wordset, gold_tokens)
    unknown_gold_tokens = ifilterfalse(lambda word, tag: word in self.wordset, gold_tokens)

    known_test_tokens = ifilter(lambda word, tag: word in self.wordset, test_tokens)
    unknown_test_tokens = ifilterfalse(lambda word, tag: word in self.wordset, test_tokens)
    # return a tuple (known_accuracy, unknown_accuracy)
    return (accuracy(known_gold_tokens, known_test_tokens),
</pre>



</div>

</div>

<div id="outline-container-3-2" class="outline-3">
<h3 id="sec-3-2"><span class="section-number-3">3.2</span> Per Tag Precision and Recall</h3>
<div class="outline-text-3" id="text-3-2">





<pre class="example">
def MicroEvaluate(self, corpus_test):
    all_tags = set([t for sent in corpus_test for (w,t) in sent])
    for tag in all_tags:
        tagged_sents = self.batch_tag([untag(sent) for sent in corpus_test])
        corpus_tokens = sum(corpus_test, []) 
        tagger_tokens = sum(tagged_sents, [])

        TP = TN = FP = FN = 0

        for tagged, gold in zip(tagger_tokens, corpus_tokens):
            wt, tt = tagged     # word_tagger, tag_tagger
            wg, tg = gold       # word_gold, tag_gold
            if wt != wg:       
                raise RuntimeError('the tagger changed the words or the word sequence '
                                   '(got {} instead of {})'.format(wt, wg))
            if tt == tag:
                if tt == tg:
                    TP += 1
                else:
                    FP += 1
            else:
                if tg == tag:
                    FN += 1
                else:
                    TN += 1

        _percision = TP / (TP + FP)
        _recall = TP / (TP + FN)
        _f_measure = (2 * TP) / (2 * TP + FP + FN)
</pre>



</div>

</div>

<div id="outline-container-3-3" class="outline-3">
<h3 id="sec-3-3"><span class="section-number-3">3.3</span> Confusion Matrix</h3>
<div class="outline-text-3" id="text-3-3">

</div>

</div>

<div id="outline-container-3-4" class="outline-3">
<h3 id="sec-3-4"><span class="section-number-3">3.4</span> SensAitivity to the Size and Structure of the Training Set: Cross-Validation</h3>
<div class="outline-text-3" id="text-3-4">

</div>

</div>

<div id="outline-container-3-5" class="outline-3">
<h3 id="sec-3-5"><span class="section-number-3">3.5</span> Stratified Samples</h3>
<div class="outline-text-3" id="text-3-5">








</div>
</div>
</div>
</div>

<div id="postamble">
<p class="date">Date: 2012-03-19 Mon</p>
<p class="author">Author: Aviad Reich, ID 052978509</p>
<p class="creator">Org version 7.8.03 with Emacs version 24</p>
<a href="http://validator.w3.org/check?uri=referer">Validate XHTML 1.0</a>

</div>
</body>
</html>
