#+TITLE:     NLP12 Assignment 1: Parts of Speech Tagging
#+AUTHOR:    Aviad Reich, ID 052978509
#+EMAIL:     avi.rei@gmail.com
#+DATE:      2012-03-19 Mon
#+DESCRIPTION:
#+KEYWORDS:
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:t toc:1-3 \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:nil pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LINK_UP:   
#+LINK_HOME: 
#+XSLT:
#+STYLE: <link rel="stylesheet" type="text/css" href="nlp.css" media="all" />

#+TEXT: [TABLE-OF-CONTENTS]

NOTES: In this assignment, the new "[[http://docs.python-requests.org/en/latest/index.html#][requests]]" python library was used,
and not =urllib=, as suggested in the tutorial. The web (and python)
have evolved since python 2.5, used in the nltk book, was released
in 2008. \\
Additionally, since =xgoogle= seemed to return very few results (1-3 on
most searches), I used [[http://breakingcode.wordpress.com/2010/06/29/google-search-python/][this script]] inst

* Data Exploration [1/2]
** DONE Gathering and cleaning up data
   CLOSED: [2012-04-30 Mon 22:44]

1) *Errors met while dealing with the Google engine:* \\
   Using the xgoogle library, only produced 1-3 result URLs for the
   searches, so I tried several alternatives: the python-duckduckgo
   library, but this only produced 1 result per query (duckduckgo only
   supplies a 'zero-click' api). Eventually, I used [[http://breakingcode.wordpress.com/2010/06/29/google-search-python/][this script]], which
   worked perfectly.
   
1) *Errors met while downloading the material from the Google hits:* \\
   I had no trouble, and was using the "requests" library.
   
1) *Errors met while cleaning up the HTML pages:* \\
   The built-in =nltk.clean_html= function did only a mediocre job
   cleaning the contents of the web pages, leaving in some irrelevant
   strings. Using the justext library worked perfectly.
   
1) *Errors met while segmenting the text into sentences and words:* \\
   Headlines were not segmented in to different sentences, but were
   instead included in the following sentence.
   The em-dash (represented as "- -", without spaces) was not regarded as separating
   between words when it should have been. For example, in: "sliding/N
   movement- -the/DET days/N of/P heaving/VG" we can see the words
   'movement' and 'the' *not* separated.

1) *Errors met by the automatic tagger:* \\
   When fed sentences that were correctly segmented to to words, and
   were also complete, and valid - I had no corrections to
   offer. Perhaps it's my lack of competence as a tagger for English
   words.. 


This is the code I used:
#+INCLUDE: "code.py" src python

In order to compare two taggers, I wrote the function:
#+INCLUDE "code2.py" src python :lines "265-277"
** Gathering basic statistics
   *IMPORTANT:* the code snippets in this section use the =tagged_words=
   corpus (NOT =tagged_sents=).


   To create Figure 1:
   
#+CAPTION: *Figure 1*
#+ATTR_HTML: width="800"
   [[file:ambiguity-fig1.png]]
   
   I used the following code:
#+INCLUDE "code2.py" src python :lines "1-41"
   
   For the requested functions, This code was used:
#+INCLUDE "code2.py" src python :lines "44-73"

** Is there a correlation between word size or frequency and ambiguity level?
   To try and answer this question, I plotted the requested 3D graph
   (figure 2a):
#+CAPTION: *Figure 2a*
#+ATTR_HTML: width="950"
[[file:ambiguity1x2.png]]

In order to better understand it, I also looked at the 3 2D
Projections of it (figure 2b), and used a logarithmic scale for word
frequency (figure 3c):
#+CAPTION: *Figure 2b - linear projections*
#+ATTR_HTML: width="950"
[[file:ambiguity2x2.png]]

#+CAPTION: *Figure 2c - logarithmic projections*
#+ATTR_HTML: width="950"
[[file:ambiguity2x2-log.png]]

Two correlations are clearly visible: *word frequency - word length* (as
discussed in class, due to "evolution" maybe), and *word ambiguity -
word length* (probably not a strictly linear correlation).\\
It looks probable, that there is also some correlation between *word
ambiguity - word frequency*, however, it's difficult to decide whether
this correlation will exist after taking into account the previous two
correlation, since they appear to be much more significant.\\
It is also worth noting that  all these observations were made with
only looking at the graphs, and might be wrong when tested for
statistical significance.


This is the code used (with minor changes or commenting
out, to create the different graphs):
#+INCLUDE "code2.py" src python :lines "76-122"
* Unigram and Affix Tagger [2/2]
** DONE Unigram Tagger
   CLOSED: [2012-05-03 Thu 09:11]
   This is the code for the Unigram Tagger:
#+INCLUDE "code2.py" src python :lines "125-151"
** DONE Affix Tagger
   CLOSED: [2012-05-03 Thu 09:11]
This is the code used:
#+INCLUDE "code2.py" src python :lines "153-253"
To produce this output (=H_param= was 0):

#+BEGIN_EXAMPLE
optimizing cutoff parameter: trying 1   ---> it scored 0.19049
optimizing cutoff parameter: trying 2   ---> it scored 0.19033
optimizing cutoff parameter: trying 3   ---> it scored 0.19027
optimizing cutoff parameter: trying 4   ---> it scored 0.19005
optimizing cutoff parameter: trying 5   ---> it scored 0.18988
optimizing cutoff parameter: trying 6   ---> it scored 0.18979
optimizing cutoff parameter: trying 7   ---> it scored 0.18963
optimizing cutoff parameter: trying 8   ---> it scored 0.18938
optimizing cutoff parameter: trying 9   ---> it scored 0.18921
optimizing cutoff parameter: trying 10  ---> it scored 0.18917
optimizing cutoff parameter: trying 15  ---> it scored 0.18848
optimizing cutoff parameter: trying 20  ---> it scored 0.18722
optimizing cutoff parameter: trying 25  ---> it scored 0.18611
optimizing cutoff parameter: trying 30  ---> it scored 0.18518
optimizing cutoff parameter: trying 35  ---> it scored 0.18367
optimizing cutoff parameter: trying 40  ---> it scored 0.18211
optimizing cutoff parameter: trying 45  ---> it scored 0.18129

choosen cutoff value is 1, which scores 0.182246809581 on the testset.
#+END_EXAMPLE

Later, I added this function to optimize the =H_param= AND the cutoff
value:
#+INCLUDE "code2.py" src python :lines "254-263"

This produced these values:
#+BEGIN_EXAMPLE
optimize_h_param: H_param=0.000000
optimizing cutoff parameter: trying  1  ---> it scored 0.19049
optimizing cutoff parameter: trying  2  ---> it scored 0.19033
optimizing cutoff parameter: trying  3  ---> it scored 0.19027
optimizing cutoff parameter: trying  4  ---> it scored 0.19005
optimizing cutoff parameter: trying  5  ---> it scored 0.18988
optimizing cutoff parameter: trying  7  ---> it scored 0.18963
optimizing cutoff parameter: trying  9  ---> it scored 0.18921
optimizing cutoff parameter: trying 10  ---> it scored 0.18917
optimizing cutoff parameter: trying 20  ---> it scored 0.18722
optimizing cutoff parameter: trying 30  ---> it scored 0.18518
optimizing cutoff parameter: trying 40  ---> it scored 0.18211
optimizing cutoff parameter: trying 50  ---> it scored 0.18028
choosen cutoff value is 1, which scores 0.181345738773 on the testset.

optimize_h_param: H_param=0.111111
optimizing cutoff parameter: trying  1  ---> it scored 0.17432
optimizing cutoff parameter: trying  2  ---> it scored 0.17416
optimizing cutoff parameter: trying  3  ---> it scored 0.17410
optimizing cutoff parameter: trying  4  ---> it scored 0.17388
optimizing cutoff parameter: trying  5  ---> it scored 0.17371
optimizing cutoff parameter: trying  7  ---> it scored 0.17346
optimizing cutoff parameter: trying  9  ---> it scored 0.17305
optimizing cutoff parameter: trying 10  ---> it scored 0.17300
optimizing cutoff parameter: trying 20  ---> it scored 0.17105
optimizing cutoff parameter: trying 30  ---> it scored 0.16902
optimizing cutoff parameter: trying 40  ---> it scored 0.16594
optimizing cutoff parameter: trying 50  ---> it scored 0.16419
choosen cutoff value is 1, which scores 0.164895957754 on the testset.

optimize_h_param: H_param=0.222222
optimizing cutoff parameter: trying  1  ---> it scored 0.16144
optimizing cutoff parameter: trying  2  ---> it scored 0.16128
optimizing cutoff parameter: trying  3  ---> it scored 0.16122
optimizing cutoff parameter: trying  4  ---> it scored 0.16100
optimizing cutoff parameter: trying  5  ---> it scored 0.16083
optimizing cutoff parameter: trying  7  ---> it scored 0.16058
optimizing cutoff parameter: trying  9  ---> it scored 0.16017
optimizing cutoff parameter: trying 10  ---> it scored 0.16012
optimizing cutoff parameter: trying 20  ---> it scored 0.15824
optimizing cutoff parameter: trying 30  ---> it scored 0.15626
optimizing cutoff parameter: trying 40  ---> it scored 0.15324
optimizing cutoff parameter: trying 50  ---> it scored 0.15193
choosen cutoff value is 1, which scores 0.151463716184 on the testset.

optimize_h_param: H_param=0.333333
optimizing cutoff parameter: trying  1  ---> it scored 0.14877
optimizing cutoff parameter: trying  2  ---> it scored 0.14861
optimizing cutoff parameter: trying  3  ---> it scored 0.14855
optimizing cutoff parameter: trying  4  ---> it scored 0.14833
optimizing cutoff parameter: trying  5  ---> it scored 0.14816
optimizing cutoff parameter: trying  7  ---> it scored 0.14791
optimizing cutoff parameter: trying  9  ---> it scored 0.14755
optimizing cutoff parameter: trying 10  ---> it scored 0.14752
optimizing cutoff parameter: trying 20  ---> it scored 0.14600
optimizing cutoff parameter: trying 30  ---> it scored 0.14449
optimizing cutoff parameter: trying 40  ---> it scored 0.14162
optimizing cutoff parameter: trying 50  ---> it scored 0.14032
choosen cutoff value is 1, which scores 0.138670606232 on the testset.

optimize_h_param: H_param=0.444444
optimizing cutoff parameter: trying  1  ---> it scored 0.13839
optimizing cutoff parameter: trying  2  ---> it scored 0.13823
optimizing cutoff parameter: trying  3  ---> it scored 0.13817
optimizing cutoff parameter: trying  4  ---> it scored 0.13795
optimizing cutoff parameter: trying  5  ---> it scored 0.13778
optimizing cutoff parameter: trying  7  ---> it scored 0.13757
optimizing cutoff parameter: trying  9  ---> it scored 0.13737
optimizing cutoff parameter: trying 10  ---> it scored 0.13733
optimizing cutoff parameter: trying 20  ---> it scored 0.13592
optimizing cutoff parameter: trying 30  ---> it scored 0.13453
optimizing cutoff parameter: trying 40  ---> it scored 0.13247
optimizing cutoff parameter: trying 50  ---> it scored 0.13129
choosen cutoff value is 1, which scores 0.129722763563 on the testset.

optimize_h_param: H_param=0.555556
optimizing cutoff parameter: trying  1  ---> it scored 0.12325
optimizing cutoff parameter: trying  2  ---> it scored 0.12308
optimizing cutoff parameter: trying  3  ---> it scored 0.12303
optimizing cutoff parameter: trying  4  ---> it scored 0.12292
optimizing cutoff parameter: trying  5  ---> it scored 0.12280
optimizing cutoff parameter: trying  7  ---> it scored 0.12260
optimizing cutoff parameter: trying  9  ---> it scored 0.12240
optimizing cutoff parameter: trying 10  ---> it scored 0.12236
optimizing cutoff parameter: trying 20  ---> it scored 0.12110
optimizing cutoff parameter: trying 30  ---> it scored 0.11981
optimizing cutoff parameter: trying 40  ---> it scored 0.11816
optimizing cutoff parameter: trying 50  ---> it scored 0.11714
choosen cutoff value is 1, which scores 0.114760797133 on the testset.

optimize_h_param: H_param=0.666667
optimizing cutoff parameter: trying  1  ---> it scored 0.10689
optimizing cutoff parameter: trying  2  ---> it scored 0.10685
optimizing cutoff parameter: trying  3  ---> it scored 0.10684
optimizing cutoff parameter: trying  4  ---> it scored 0.10678
optimizing cutoff parameter: trying  5  ---> it scored 0.10667
optimizing cutoff parameter: trying  7  ---> it scored 0.10648
optimizing cutoff parameter: trying  9  ---> it scored 0.10636
optimizing cutoff parameter: trying 10  ---> it scored 0.10634
optimizing cutoff parameter: trying 20  ---> it scored 0.10531
optimizing cutoff parameter: trying 30  ---> it scored 0.10454
optimizing cutoff parameter: trying 40  ---> it scored 0.10313
optimizing cutoff parameter: trying 50  ---> it scored 0.10246
choosen cutoff value is 1, which scores 0.100417007188 on the testset.

optimize_h_param: H_param=0.777778
optimizing cutoff parameter: trying  1  ---> it scored 0.08989
optimizing cutoff parameter: trying  2  ---> it scored 0.08989
optimizing cutoff parameter: trying  3  ---> it scored 0.08989
optimizing cutoff parameter: trying  4  ---> it scored 0.08984
optimizing cutoff parameter: trying  5  ---> it scored 0.08974
optimizing cutoff parameter: trying  7  ---> it scored 0.08958
optimizing cutoff parameter: trying  9  ---> it scored 0.08950
optimizing cutoff parameter: trying 10  ---> it scored 0.08947
optimizing cutoff parameter: trying 20  ---> it scored 0.08878
optimizing cutoff parameter: trying 30  ---> it scored 0.08831
optimizing cutoff parameter: trying 40  ---> it scored 0.08693
optimizing cutoff parameter: trying 50  ---> it scored 0.08648
choosen cutoff value is 3, which scores 0.08349573563 on the testset.

optimize_h_param: H_param=0.888889
optimizing cutoff parameter: trying  1  ---> it scored 0.06796
optimizing cutoff parameter: trying  2  ---> it scored 0.06796
optimizing cutoff parameter: trying  3  ---> it scored 0.06796
optimizing cutoff parameter: trying  4  ---> it scored 0.06794
optimizing cutoff parameter: trying  5  ---> it scored 0.06786
optimizing cutoff parameter: trying  7  ---> it scored 0.06770
optimizing cutoff parameter: trying  9  ---> it scored 0.06762
optimizing cutoff parameter: trying 10  ---> it scored 0.06759
optimizing cutoff parameter: trying 20  ---> it scored 0.06704
optimizing cutoff parameter: trying 30  ---> it scored 0.06663
optimizing cutoff parameter: trying 40  ---> it scored 0.06548
optimizing cutoff parameter: trying 50  ---> it scored 0.06522
choosen cutoff value is 3, which scores 0.0618281259823 on the testset.

#+END_EXAMPLE
We can clearly see that cutting off according to ANY criteria, causes
a degradation in the tagger's performance, except for the case when
=H_param= was high ($\frac{7}{9}$, $\frac{8}{9}$), where cutoff values
of 1-3 are equally good.


1) *Does entropy filtering improve accuracy?*
   It does not (!!)

2) *How do you determine the range of values to test for the cutoff?*
   I used the values 1-5, 7,9, 10, 20, 30, 40, 50. This was not taken
   from the corpus, but seemed to give more than reasonable cover for
   several corpora.
  
3) *Is the accuracy value evolving in a predictable manner as the cutoff varies?*
   It decreases as the cutoff increases.

4) *Describe the list of suffixes that are good tag predictors -- are
   you surprised by what you observe?*


#+BEGIN_EXAMPLE
eir gives 99.963 percent prediction as PRO
uld gives 99.942 percent prediction as MOD
ods gives 99.735 percent prediction as N
rld gives 99.624 percent prediction as N
iam gives 99.444 percent prediction as NP
ths gives 99.413 percent prediction as N
tel gives 99.379 percent prediction as N
ups gives 99.363 percent prediction as N
ror gives 99.306 percent prediction as N
cil gives 99.296 percent prediction as N
're gives 99.288 percent prediction as PRO
rgy gives 99.180 percent prediction as N
hip gives 99.135 percent prediction as N
tem gives 99.057 percent prediction as N
ege gives 98.997 percent prediction as N
tee gives 98.985 percent prediction as N
umn gives 98.936 percent prediction as N
try gives 98.825 percent prediction as N
ogy gives 98.805 percent prediction as N
hor gives 98.780 percent prediction as N
ity gives 98.778 percent prediction as N
dex gives 98.765 percent prediction as N
arp gives 98.649 percent prediction as ADJ
hod gives 98.611 percent prediction as N
yne gives 98.485 percent prediction as NP
not gives 98.473 percent prediction as MOD
nst gives 98.425 percent prediction as P
ons gives 98.391 percent prediction as N
oms gives 98.378 percent prediction as N
elf gives 98.328 percent prediction as PRO
ife gives 98.230 percent prediction as N
bol gives 98.214 percent prediction as N
gth gives 98.127 percent prediction as N
dar gives 98.077 percent prediction as N
lue gives 98.058 percent prediction as N
wly gives 98.000 percent prediction as ADV
sis gives 97.950 percent prediction as N
hts gives 97.947 percent prediction as N
rey gives 97.917 percent prediction as NP
ski gives 97.778 percent prediction as NP
ffs gives 97.778 percent prediction as N
ols gives 97.612 percent prediction as N
ool gives 97.543 percent prediction as N
#+END_EXAMPLE

I can see why some of these would be good predictors, especially for
the Nouns I (think I) recognize, such as "ool", "ols", "ogy", and why "'re",
but for most of them - I can't really imagine why they are so good.
I have to admit that I don't recognize any of these as a suffix I
studies in linguistics.

* Fine-grained Error Analysis [3/5]
** DONE Known vs. Unknown Accuracy
   CLOSED: [2012-05-03 Thu 01:29]

   I propose to identify known words as words that appeared more than
   $N$ times in the training corpus, where $N>0$ (I'll use $N=1$, but that's just
   one option).

   I suggest adding the code (for $N=1$): \\
   ~self.wordset = set([word for sentence in train for (word, tag) in sentence])~
   
   To every tagger. This could be used in the =evaluate2= method, in
   the following manner:
#+INCLUDE "/home/lxmonk/git/nltk/nltk/tag/api.py" src python :lines "70-87"


** DONE Per Tag Precision and Recall
   CLOSED: [2012-05-03 Thu 10:21]

#+INCLUDE "/home/lxmonk/git/nltk/nltk/tag/api.py" src python :lines "88-129"


** TODO Confusion Matrix
   The data structure for this is the ConditionalFreqDist. \\
   The code (in =api.py=):
#+INCLUDE "/home/lxmonk/git/nltk/nltk/tag/api.py" src python :lines "129-146"
   Results for brown corpus (simplified tags):
#+BEGIN_EXAMPLE

#+END_EXAMPLE

** DONE Sensitivity to the Size and Structure of the Training Set: Cross-Validation
   CLOSED: [2012-05-03 Thu 22:42]
   The code for the function:
#+INCLUDE "code2.py" src python :lines "153-169"
   Produced:
#+BEGIN_EXAMPLE
In [12] t0 = nltk.DefaultTagger('NN')
In [13] t1 = nltk.UnigramTagger(nltk.corpus.treebank.tagged_sents(simplify_tags=True), backoff=t0)
In [14] t2 = nltk.BigramTagger(nltk.corpus.treebank.tagged_sents(simplify_tags=True), backoff=t1)
In [15] table = t2.crossValidate(nltk.corpus.brown.tagged_sents(simplify_tags=True), 10)

 i  accuracy(i)
 --------------
 0)   0.135158702476
 1)   0.483606557377
 2)   0.486920125567
 3)   0.472096267876
 4)   0.460062783397
 5)   0.482211370771
 6)   0.514475061039
 7)   0.571154516917
 8)   0.56853854203
 9)   0.55284269271

In [16]: average(table.values())
Out[16]: 0.47270666201604455
#+END_EXAMPLE
   We can see that the accuracy of the 0^{th} iteration is very low in
   comparison to the others. Ignoring it produces:
#+BEGIN_EXAMPLE
In [18]: average([table[i] for i in range(1,10)])
Out[18]: 0.51021199085377655
#+END_EXAMPLE
   Which is not very different. This, however, shows the importance of
   covering as much of the training set as possible, since there might
   be significant variations.
      

** TODO Stratified Samples
   To define sentence lengths, I used:
#+BEGIN_SRC python
fd = nltk.FreqDist([len(sent) for sent in nltk.corpus.brown.tagged_sents()])
hist(list(chain.from_iterable([[k]*fd[k] for k in fd.keys()])),
     bins=(max(fd.keys()) - min(fd.keys()) + 1))
#+END_SRC

   The histogram is this:
#+ATTR_HTML: width="950"
[[file:hist_3.5.png]]

   I choose the lengths of $short \leq 12$ (short, after first peak), 
   $12 < medium \leq 23$ (medium), and $23 < long$ (long).
   Looking at the numbers, we see it's the integer equivalent of
   dividing to 3 equal groups:
   
\begin{equation}
\sum_{i=0}^{\infty} fd[i] = 57340,\: \sum_{i=0}^{12} fd[i] = 18167 \approx
\frac{1}{3} \cdot 57340,\: \sum_{i=0}^{23} fd[i] = 38322 \approx \frac{2}{3}
\cdot 57340

\end{equation}


